name: PKD_SLT
model_dir: "SLT-Transformer"
use_cuda: True
fp16: True
random_seed: 42

data:
    data_path: ./data/
    dataset_type: "plain"
    version: PDK-SLT1.0
    sgn: sign
    txt: text
    gls: gloss
    train: data/train_data.pkl.train
    dev: data/dev_data.pkl.dev
    test: data/test_data.pkl.test
    feature_size: 783
    level: word
    txt_lowercase: true
    max_sent_length: 400
    random_train_subset: -1
    random_dev_subset: -1
    special_symbols: # special symbols
        unk_token: "<unk>"          # surface form for unknown token
        pad_token: "<pad>"          # surface form for pad token
        bos_token: "<s>"            # surface form for begin-of-sentence token
        eos_token: "</s>"           # surface form for end-of-sensente token
        #sep_token: "<sep>"          # surface form for separator; Used for prompt
        unk_id: 0                   # unknown token index
        pad_id: 1                   # pad token index
        bos_id: 2                   # begin-of-sentence token index
        eos_id: 3                   # end-of-sensence token index
        #sep_id: 4                   # separator token index
        #lang_tags: ["<de>", "<en>"] # language tags; Used for multi-task training
    src:
        level: sgn
        lang: sign
    trg:
        level: word
        lang: de
        lowercase: True
        normalize: True
        max_length: 30
        min_length: 1




training:
    overwrite: True
#    num_workers: 0

testing:
    nun: 1

model:
    model: True
    encoder:
        type: transformer
        num_layers: 3
        num_heads: 8
        embedding:
            embedding_dim: 512
            scale: false
            dropout: 0.1
            norm_type: batch
            activation_type: softsign
        hidden_size: 512
        ff_size: 2048
        dropout: 0.1
        layer_norm: "pre"
        activation: "relu"
    decoder:
        type: transformer
        num_layers: 4
        num_heads: 8
        embedding:
            embedding_dim: 512
            scale: false
            dropout: 0.1
            norm_type: batch
#            activation_type: softsign  #词嵌入不需要激活函数
        hidden_size: 512
        ff_size: 2048
        dropout: 0.1
        layer_norm: "pre"
        activation: "relu"


